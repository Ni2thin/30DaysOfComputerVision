# 30 Days of Computer Vision ðŸš€

![Last Commit](https://img.shields.io/github/last-commit/Ni2thin/30DaysOfComputerVision)
![Repo Size](https://img.shields.io/github/repo-size/Ni2thin/30DaysOfComputerVision)

Like the rising sun, every new day brings an opportunity to shine brighter. With patience, determination, and resilience, we will paint our path in the world of computer vision. **ä¸ƒè»¢ã³å…«èµ·ã** (Nanakorobi yaoki) â€“ "Fall seven times, stand up eight."



## Resources and Progress ðŸ“š

| Books & Resources                                   | Completion Status | 
|-----------------------------------------------------|-------------------|
| [Machine Learning Specialization on Coursera](https://www.coursera.org/specializations/machine-learning-introduction) | ðŸ“ˆ        |      
| [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) |   ðŸ“ˆ   |      
| [Computer Vision YouTube Playlist](https://www.youtube.com/watch?v=HiTw5KFw7ic&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd) |  ![Playlist Thumbnail](https://img.youtube.com/vi/HiTw5KFw7ic/hqdefault.jpg) |

## Progress Tracker

| Day  | Date         | Topics                                               | Resources                                          |
|------|--------------|------------------------------------------------------|----------------------------------------------------|
| Day 1 | 27-12-2024  | OpenCV tutorial for beginners                        | [OpenCV tutorial](https://www.youtube.com/watch?v=eDIj5LuIL4A&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=2)     |
| Day 2 | 28-12-2024  | Detecting color with Python and OpenCV               | [Detecting color](https://www.youtube.com/watch?v=aFNDh5k3SjU&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=4)                   |
| Day 3 | 29-12-2024  | Face detection and blurring with Python              | [Face detection and blurring](https://www.youtube.com/watch?v=DRMBqhrfxXg&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=4)       |
| Day 4 | 30-12-2024  | Text detection with Python(Tesseract)             | [Text detection with Python](https://www.youtube.com/watch?v=CcC3h0waQ6I&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=5)        |
| Day 5 | 31-12-2024  | Image classification with Python and OpenCV          | [Image classification with Python](https://www.youtube.com/watch?v=il8dMDlXrIE&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=6&pp=iAQB)  |
| Day 6 | 01-01-2025  | Emotion detection with Python, OpenCV, and others    | [Emotion detection](https://www.youtube.com/watch?v=h0LoewzGzhc&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=8)                 |
| Day 7 | 02-01-2025  | Image classification + feature extraction           | [Image classification + feature extraction](https://youtu.be/oEKg_jiV1Ng?feature=shared) |
| Day 8 | 03-01-2025  | Sign language detection with Python and OpenCV       | [Sign language detection](https://youtu.be/MJCSjXepaAM?feature=shared)           |
| Day 9 | 04-01-2025  | Image classification WEB APP with Python & Flask     | [Image classification web app](https://youtu.be/n_eMARPqBZI?feature=shared)      |
| Day 10 | 05-01-2025 | AWS Rekognition tutorial (Object detection)      | [AWS Rekognition](https://www.youtube.com/watch?v=C9H_v44670s)                   |
| Day 11 | 06-01-2025 | Yolov8 object tracking 100% native                   | [Yolov8 object tracking](https://youtu.be/uMzOcCNKr5A?feature=shared)            |
| Day 12 | 07-01-2025 | Image segmentation with Yolov8 and Segment Anything Model(SAM)  | [Image segmentation with Yolov8](https://www.youtube.com/watch?v=aYP4ujUsGdk)    |
| Day 13 | 08-01-2025 | Pose detection using webcam      | [Train pose detection ](https://www.youtube.com/watch?v=06TE_U21FK4)       |
| Day 14 | 09-01-2025 | Parking spot detection and counter                   | [Parking spot detection](https://www.youtube.com/watch?v=F-884J2mnOY&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=15&pp=iAQB)            |
| Day 15 | 10-01-2025 | Train Yolov10 object detection custom dataset        | [Train Yolov10 object detection](https://www.youtube.com/watch?v=PfQwNe0P-G4)    |
| Day 16 | 11-01-2025 | End to end pipeline real world computer vision       | [End to end pipeline](https://youtu.be/xgtujvjKIGs?feature=shared)               |
| Day 17 | 12-01-2025 | Image processing API with AWS API Gateway            | [Image processing API](https://www.youtube.com/watch?v=iUoiBRwyIBg)              |
| Day 18 | 13-01-2025 | How much data you need to train a computer vision model | [How much data to train cv](https://youtu.be/8YXk_zcllC8?feature=shared)                 |
| Day 19 | 14-01-2025 | Real world application of computer vision           | [Real world application](https://youtu.be/OP8AozaEuLM?feature=shared)            |
| Day 20 | 15-01-2025 | Train Detectron2 object detection custom dataset     | [Train Detectron2](link)                  |
| Day 21 | 16-01-2025 | Face recognition on your webcam with Python & OpenCV | [Face recognition](link)                  |
| Day 22 | 17-01-2025 | Face attendance + face recognition with OpenCV      | [Face attendance](link)                   |
| Day 23 | 18-01-2025 | Machine learning with AWS practical projects        | [ML with AWS](link)                       |
| Day 24 | 19-01-2025 | Chat with an image (LangChain custom)                | [Chat with an image](link)                |
| Day 25 | 20-01-2025 | Image generation with Python (Train Stable Diffusion) | [Image generation](link)                  |
| Day 26 | 21-01-2025 | Face recognition + liveness detection               | [Face recognition + liveness](link)       |
| Day 27 | 22-01-2025 | Face recognition and face matching with Python      | [Face matching](link)                     |
| Day 28 | 23-01-2025 | Machine learning web app with Python, Flask, and others | [ML web app](link)                     |
| Day 29 | 24-01-2025 | Object detection on Raspberry Pi USB camera         | [Object detection on Raspberry Pi](link)  |
| Day 30 | 25-01-2025 | Image generation with Python & Stable Diffusion     | [Image generation](link)                  |

---

# Day 1: Getting started with OpenCV

## Key Concepts

- **Images in CV2**: Images are stored as **NumPy arrays** in OpenCV.
- **Image Shape**: The shape of an image consists of **height, width, and channels** (BGR format).
- **Color Format**: 
  - OpenCV uses **BGR** (Blue, Green, Red) while libraries like Matplotlib use **RGB**.
  - To convert from BGR to RGB, use:
    ```
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    ```
- [Introduction to CV2 ](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/CV2%20Intro)   

## Image Manipulation

- **Shape and Resize**: Use `cv2.resize()` to change the dimensions of an image.
- **Cropping**: Crop images by slicing the NumPy array.

## Color Space Conversions

- Convert between color spaces using functions like:
  - `cv2.cvtColor(image, cv2.COLOR_BGR2RGB)` for BGR to RGB
  - `cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)` for BGR to Grayscale
  - `cv2.cvtColor(image, cv2.COLOR_BGR2HSV)` for BGR to HSV

## Blurring Techniques

- Use various blurring methods such as:
  - `cv2.blur()`
  - `cv2.medianBlur()`
  - `cv2.GaussianBlur()`

## Thresholding Methods

- Apply different thresholding techniques for segmentation:
  - **Global Thresholding**
  - **Adaptive Thresholding**
  - Converts images from normal to binary.

## Edge Detection Techniques

- Utilize edge detection algorithms like:
  - `cv2.Sobel()`
  - `cv2.Canny()`
  - `cv2.Laplacian()`
- Morphological operations such as dilation and erosion can also be applied.

## Drawing Functions

- Draw shapes on images using functions like:
  - `cv2.line()`
  - `cv2.rectangle()`
  - `cv2.circle()`

## Contours

### Features:
- **Grayscale Conversion**: Converts the input image to grayscale for easier processing.
- **Thresholding**: Applies binary inverse thresholding to create a binary image suitable for contour detection.
- **Contour Extraction**: Uses OpenCV's `findContours` to extract object contours from the thresholded image.
- **Bounding Boxes**: Calculates and draws bounding rectangles around contours exceeding a specified area threshold.
- **Dynamic Visualization**: Displays the original image with bounding boxes and the thresholded binary image for analysis.

  ---
  
# Day 2: Detecting color with Python and OpenCV 

This project uses OpenCV to detect and track objects of a specific color in real-time using a webcam feed. It processes frames to isolate the target color, highlights detected objects with bounding boxes, and displays the result. [color detection](https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Projects/color-detection.py)

## Features

- Dynamic HSV Range:
Adapts HSV thresholds for any BGR color, useful for varying lighting conditions.
- Hue Wrapping for Red:
Handles HSV hue wrapping (0â€“10 and 170â€“180) to avoid missing red objects.
- Noise Reduction:
Combines GaussianBlur and morphological operations (MORPH_CLOSE, MORPH_OPEN) for clean masks.
- Real-Time Optimization:
Ignores small noise with contour area thresholding (area > 500).
- Mirror-Like View:
Flips frames (cv2.flip) for intuitive user interaction.
- Dynamic Highlighting:
Draws bounding boxes around detected objects; replace with contours for precise outlines.
- Fail-Safe Video Capture:
Handles webcam failures gracefully with if not ret.
- Multi-Color Support:
Extendable to detect multiple colors by blending masks.

---

# Day 3: Face Anonymizer  

This project implements face detection and blurring using **OpenCV** and **Mediapipe**. The application supports three different modes for processing images and video.
[Face Anonymizer output](Images/face-blur.png)

### Key Features:
- **Face Detection Model**: Uses Mediapipe's face detection model with a **50% confidence threshold** to detect faces.
- **Blurring**: A Gaussian blur with a kernel size of **(30, 30)** is applied to the detected faces.
- **Video Processing**: Processes video frames at **25 FPS** and saves them in an output directory.
- **Output**: All processed images and videos are saved in the **output** directory.
  
---

# Day 4: Text detector using tesseract

## **Features**
1. **Text Detection in Images**
   - Processes an input image to detect and extract text.
   - Highlights detected text regions with bounding boxes.
   
2. **Real-Time Text Detection via Webcam**
   - Captures webcam feed to detect and extract text in real-time.
   - Displays processed frames with detected text.


## **Key Concepts**
1. **Preprocessing Techniques**
   - Convert images to grayscale.
   - Apply thresholding to enhance text regions.
   - Use morphological operations (dilation) to emphasize text.

2. **Text Detection**
   - Use Tesseract OCR with custom configurations:
     - **OEM 3**: Tesseract engine mode for both legacy and LSTM models.
     - **PSM 6**: Page segmentation mode for detecting text blocks.
   - Extract text data (including bounding box coordinates) using `pytesseract.image_to_data`.

3. **Bounding Boxes**
   - Draw rectangles around detected text regions with confidence > 50%.
   
---

# Day 5: Parking Spot Detection and Classification

This project leverages Python and OpenCV to detect and classify parking spots in real-time. It processes video feeds or images to identify available and occupied parking spots,employing advanced image processing techniques to provide efficient parking management solutions. [Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/image-classification)

## **Features**
1. **Parking Spot Detection**  
   - Detect parking spots in video using a binary mask and extract regions of interest (ROIs).

2. **Real-Time Spot Classification**  
   - Classify each spot as `empty` or `not_empty` using a pre-trained SVM model.  
   - Update status dynamically and visualize with:
     - **Green** bounding boxes for `empty` spots.  
     - **Red** bounding boxes for `not_empty` spots.

3. **Availability Counter**  
   - Displays the number of available spots in real-time on the video feed.

4. **Efficient Frame Processing**  
   - Analyze frames at intervals to optimize performance while ensuring accuracy.

### **Output**
<img src="https://github.com/Ni2thin/30DaysOfComputerVision/blob/04c4f59c6f005878688dd18dac90a65fe234c853/Projects/image-classification/Parking%20spot%20detection/parking%20spot%20-%20output.png" width="1050" height="500"/>

### **Assets** 
[Download Dataset](https://drive.google.com/drive/folders/15lLq-6Bbuq7LyILMg2rzJOL4WpnxX6oX?usp=share_link) 

---
# Day 6: Emotion Detection with Python and OpenCV  

This project utilizes Python, OpenCV, and face landmark detection to classify human emotions from images. It processes face images, extracts 1404 key landmarks, and organizes them into labeled datasets for emotion classification. [Emotion detection project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Emotion%20detection)  

## **Features**  

- **Emotion Categorization**:  
  Processes images into seven predefined emotional categories (e.g., happy, sad, angry).  

- **Facial Landmark Extraction**:  
  Uses a custom `get_face_landmarks` function to capture precise facial landmarks essential for emotion detection.  

- **Batch Processing**:  
  Efficiently processes images in batches to optimize resource usage and avoid system overloads.  

- **Intermediate Data Storage**:  
  Saves labeled datasets for each emotion in separate text files (`data_<emotion>.txt`) and combines them into a single dataset (`data.txt`).  

- **Robust File Handling**:  
  Skips invalid or non-image files to ensure smooth processing.  

- **Scalable Architecture**:  
  Easily extendable to support more emotions or larger datasets without requiring significant code changes.  

- **Threading Optimization**:  
  Limits system thread usage to prevent crashes and improve stability, especially on macOS.  

---
# Day 7: Image Classification and Feature Extraction with Python  

This project leverages `Img2Vec` and a pre-trained Random Forest model to classify images into predefined categories. It extracts feature vectors from images, predicts their labels, and provides confidence scores for each classification. [Image classification and feature extraction project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/img%20classification%20and%20extraction)

## **Features**  

- **Image Feature Extraction**:  
  Utilizes `Img2Vec` to generate 512-dimensional feature vectors, representing high-level characteristics of images.  

- **Robust Classification**:  
  Employs a Random Forest classifier to predict image labels with high accuracy.  

- **Confidence Scoring**:  
  Outputs confidence scores alongside predictions to gauge the reliability of the results.  

- **Error Handling**:  
  Includes checks for file existence and compatibility to prevent crashes during runtime.  

- **Human-Readable Output**:  
  Converts predictions into category labels (e.g., "Sunny", "Cloudy") for better interpretability.  

- **Flexible Input**:  
  Compatible with various image formats (e.g., JPG, PNG, BMP).  

- **Modular Workflow**:  
  Easily extendable to add more categories or integrate advanced models for improved classification.  

---
# Day 8: Building a Sign Language Detector with Python and OpenCV

This project implements a real-time sign language detection system using OpenCV, MediaPipe, and a machine learning model. The system captures hand gestures via a webcam, processes them to extract features, and predicts corresponding sign language characters. 
[Sign Language Detection Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Sign%20language%20detection)


## Features

- **Dataset Creation**: Captures 100 images per class using the webcam for generating a labeled dataset.
- **Feature Extraction**: Utilizes MediaPipe's hand landmarks to extract normalized hand features (x, y coordinates).
- **Model Training**: Trains a Random Forest Classifier on the extracted features to distinguish between gestures.
- **Real-Time Prediction**: Detects hand gestures in live webcam feed and overlays the predicted character on the video stream.
- **Visualization**: Draws hand landmarks and bounding boxes around detected hands for intuitive visualization.
- **Expandable Design**: Easy to extend with additional gestures or classes by adding new labeled data.

## Workflow

### 1. Data Collection
- Webcam feed captures images for each gesture class.
- Images are saved to class-specific directories.

### 2. Model Training
- Extracts hand landmarks and normalizes them.
- Trains a Random Forest model on the features for classification.

### 3. Real-Time Detection
- Processes live webcam feed to detect and classify gestures.
- Displays predictions with bounding boxes and labels.

## Key Innovations

- **Hand Landmarks**: MediaPipe's hand tracking ensures accurate gesture representation.
- **Efficient Dataset Handling**: Automates data collection and preprocessing for rapid model training.
- **Dynamic Prediction**: Real-time responsiveness allows seamless interaction with the system.
- **Scalable Architecture**: Can be extended to support more complex sign language alphabets.

## How to Use

1. **Dataset Creation**:
   - Run the data collection script to capture hand gesture images.
   - Ensure each gesture has its own labeled folder in the dataset directory.

2. **Model Training**:
   - Run the training script to train the Random Forest Classifier.
   - A trained model will be saved for later use.

3. **Real-Time Detection**:
   - Launch the detection script to start the webcam feed.
   - The system will predict and display the corresponding gesture in real-time.

## Example Output

The system detects gestures and overlays predictions directly on the live video feed:
<p align="center">
  <img src="Projects/Sign%20language%20detection/outputs/B.png" alt="Image 1" height="300px">
  <img src="Projects/Sign%20language%20detection/outputs/A.png" alt="Image 2" height="300px">
</p>

---
# Day 9: Pneumonia Classification - Simple web app

This project utilizes Python and Keras to classify chest X-ray images as either showing signs of pneumonia or being normal. The model is trained using the Teachable Machine platform, and the dataset is taken from Kaggle's Chest X-Ray dataset. The goal is to assist in the identification of pneumonia cases using deep learning. [Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Pneumonia%20classification)

## **Features**
1. **Pneumonia Detection**  
   - Classify chest X-ray images into two categories: `Pneumonia` and `Normal`.
   - Use pre-trained models fine-tuned with a custom dataset to provide accurate results.

2. **Prediction Confidence**  
   - Displays the model's confidence in the prediction as a percentage.
   - Provide a progress bar for an interactive user experience.

3. **Real-Time Upload and Prediction**  
   - Upload a chest X-ray image and get an immediate prediction for pneumonia classification.
   - Show prediction results on the app with a confidence score.

4. **User-Friendly Interface**  
   - The web app is designed with a simple and intuitive UI, allowing users to easily upload and classify chest X-ray images.
   - Streamlit is used to build an interactive and seamless user experience.

### **Output**
<p align="center">
  <img src="Projects/Pneumonia classification/output/output.png" alt="Image 1" height="300px">
  <img src="Projects/Pneumonia classification/output/output2.png" alt="Image 2" height="300px">
</p>

### **Assets**
- [Download Dataset from Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data)
- [Teachable Machine](https://teachablemachine.withgoogle.com/)
---
# Day 10: Object detection using AWS Rekognition 

This project demonstrates the use of AWS Rekognition to detect objects in a video stream. Specifically, it identifies instances of a target class (`Zebra` in this example) and saves the bounding box annotations and corresponding frames. The implementation leverages AWS Rekognition's label detection capabilities to process video frames and extract object information.

## **Features**
1. **Frame Processing and Object Detection**
   - Reads video frames using OpenCV.
   - Sends each frame to AWS Rekognition for object detection.
   - Identifies and processes instances of the target class (`Zebra`).

2. **Bounding Box Annotation**
   - Extracts bounding box details (center coordinates, width, and height) for the detected objects.
   - Saves annotations in YOLO-compatible format as text files for each frame.

3. **Frame Output Storage**
   - Saves each processed video frame as an image in a designated directory.
   - Frames are named sequentially for easy reference.

4. **Customizable Target Class**
   - Allows users to specify a different target object class by modifying the `target_class` variable.

5. **AWS Rekognition Integration**
   - Utilizes AWS Rekognitionâ€™s `detect_labels` API for high-confidence object detection.
   - Customizable minimum confidence threshold for detections.

## **Usage**
### Prerequisites
1. AWS Rekognition credentials:
   - Ensure you have an active AWS account with access to Rekognition services.
   - Replace the placeholders in `credentials.py` with your AWS Access Key and Secret Key:
     ```python
     access_key = "YOUR_AWS_ACCESS_KEY"
     secret_key = "YOUR_AWS_SECRET_KEY"
     ```

2. Install the required Python libraries:
   ```bash
   pip install boto3 opencv-python
   ```
   --- 
# Day 11: Object Detection with YOLO and OpenCV

This project demonstrates how to use the YOLO (You Only Look Once) model from the Ultralytics library to perform real-time object detection on a video file. It processes video frames, applies YOLO for object detection, and displays the results with bounding boxes and annotations. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Object%20detection%20using%20YOLO)

## Features

- **Pretrained YOLO Models**: Utilizes the YOLOv8 model (`yolov8n.pt`) for lightweight and fast object detection.  
- **Video File Input**: Processes video frames from a local file (`cat.mp4`).  
- **Real-Time Detection**: Performs real-time object detection on each frame with high accuracy.  
- **Dynamic Visualization**: Displays detected objects with bounding boxes and labels overlaid on the video.  
- **Customizable Models**: Replace `yolov8n.pt` with other YOLO weights (`yolov8s.pt`, `yolov8m.pt`, etc.) for higher precision.  
- **Fail-Safe Video Handling**: Checks and handles errors if the video file cannot be opened or frames cannot be read.  
- **Interactive Interface**: Press `q` to exit the visualization during playback.

## Prerequisites

- Python 3.8 or later
- Install required libraries:
  ```bash
  pip install ultralytics opencv-python
  ```
---
# Day 12: Image segmentation with Yolov8 and Segment Anything Model(SAM) 

This project demonstrates how to detect and segment objects in an image using **YoloV8** for object detection, **Segment Anything Model (SAM)** for segmentation, and finally removing the background. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Image%20segmentation%20using%20yolov8%20and%20SAM)

## Steps

### 1. Install Dependencies

Install YoloV8 and SAM, and download the SAM model checkpoint.
```bash
!pip install ultralytics
!pip install 'git+https://github.com/facebookresearch/segment-anything.git'
!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
```
### 2. Object Detection with YoloV8

Detect objects in the image.

```bash
!yolo predict model=yolov8n.pt source='your-image.jpeg'
```
- visualize detection
  ``` python
  Image(filename='/content/runs/detect/predict/your-image.jpg', height=600)
  ```
### 3. Segmentation with SAM

Use SAM to segment objects based on YoloV8 bounding boxes.
Visualize segmentation results:

```python
masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)
plt.imshow(image)
show_mask(masks[0], plt.gca())
plt.show()
```
### 4. Background Removal

Create a binary mask and isolate the object:

``` python
binary_mask = np.where(segmentation_mask > 0.5, 1, 0)
new_image = (255 * (1 - binary_mask[..., None]) + image * binary_mask[..., None])
plt.imshow(new_image.astype(np.uint8))
plt.axis('off')
plt.show()
```
---
# Day 13: Pose Detection and Rep Counter

## Overview
This project utilizes **OpenCV** and **MediaPipe** to perform real-time pose detection and calculate the angles of arm movements for fitness tracking. The system detects arm curls, tracks the number of repetitions for each arm, and provides a live visual feed with overlaid information. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/AI%20Pose%20Estimation%20for%20gym)

## Features
- **Real-Time Pose Detection:** Detects human poses using MediaPipe's Pose solution.
- **Angle Calculation:** Calculates joint angles to determine arm movement stages (e.g., "up" or "down").
- **Repetition Counter:** Tracks and counts curls for both left and right arms.
- **Camera Flipping:** Mirrors the video feed for a natural user experience.
- **On-Screen Visuals:**
  - Real-time joint angles.
  - Left and right arm repetition counts.
  - Frames Per Second (FPS).
  - Instructions for pausing or quitting.
- **Data Logging:** Saves the final repetition counts to a text file (`reps_data.txt`).

## Curl Logic
The repetition counter uses the calculated angle of the elbow joint to determine the arm's movement stages ("up" or "down") and to count completed curls.

1. **Landmark Detection:**
   - Extract the coordinates of the shoulder, elbow, and wrist for each arm using MediaPipe landmarks.

2. **Angle Calculation:**
   - Calculate the angle at the elbow joint using the formula:
      angle = | atan2(c_y - b_y, c_x - b_x) - atan2(a_y - b_y, a_x - b_x) | Ã— 180/Ï€

     where:
     - `a`, `b`, and `c` are the shoulder, elbow, and wrist coordinates, respectively.

3. **Movement Stages:**
   - If the angle exceeds 160Â°, the arm is considered **"down"** (extended).
   - If the angle drops below 30Â° and the arm was previously "down," the arm is considered **"up"** (flexed).

4. **Increment Counter:**
   - Transitioning from "down" to "up" increments the repetition counter.

## Instructions
1. Run the script.
2. Perform arm curls in front of the camera.
3. View real-time feedback, including repetitions and angles.
4. Press `P` to pause or `Q` to quit.
5. Check the `reps_data.txt` file for a summary of your session.


## Use Cases
- Fitness tracking for arm workouts.
- Pose analysis for personal or professional fitness training.
- Real-time feedback for improving exercise form.

This project provides a foundation for fitness applications with advanced pose estimation features.

---
# Day 14: Parking Spot Detection System

A computer vision-based solution to detect parking spot availability, designed for versatile use cases, including Indian parking scenarios [link.](https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Projects/image-classification/Parking%20spot%20detection/improved-parking-spot-detection.py)

## Features

- **Dynamic Configuration**: Use command-line arguments for mask and video paths.
- **Optimized Processing**: Configurable frame skipping (`--step`) and sensitivity (`--diff_threshold`).
- **Indian Context Support**: Handles compact spaces, irregular layouts, and lighting variations.
- **Real-Time Visualization**: Color-coded bounding boxes for parking spots.
- **Modular Design**: Easily adaptable for live feeds or IoT integration.

## Installation

Clone the repository:
   ```bash
   git clone https://github.com/Ni2thin/30DaysOfComputerVision.git
   cd Projects/image-classification/Parking spot detection/improved-parking-spot-detection.py
```
## Command-Line Arguments

| Argument          | Description                                    | Default    |
|--------------------|------------------------------------------------|------------|
| `--mask`          | Path to the binary mask image.                | Required   |
| `--video`         | Path to the video file.                       | Required   |
| `--step`          | Frame processing interval.                    | `30`       |
| `--diff_threshold`| Threshold for detecting spot changes.         | `0.4`      |

## Output

- **Visualization**: 
  - Green boxes for available spots, red for occupied.
- **Console Logs**: Spot change metrics and parking availability.

## Limitations

- **Lighting Sensitivity**: Adjust `--diff_threshold` for shadows or glare.
- **Mask-Video Alignment**: Ensure perfect alignment for accurate detection.
- **Camera Angle**: Best results with top-down or high-angle views.
---
# Day 15: Object detection using YOLOv10 with custom data

 This project demonstrates how to build a custom object detector using YOLOv10, from dataset creation to deploying a functional model. It emphasizes the iterative nature of training and evaluation to achieve reliable real-world performance. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Object%20detection%20using%20YOLOv10)

## key steps:  
- Data collection  
- Data annotation  
- Preparing the dataset  
- Training the model  
- Evaluating the model  
- Making predictions  

 Here the dataset focuses on images of ducks, demonstrating the importance of a structured approach to train an effective object detection model.

## Data Collection  
- **Dataset Description**: Images of ducks in various sizes, colors, and backgrounds.  
- **Importance**: A large and diverse dataset enhances the model's generalization capabilities.  
- **Recommendations**: Collect hundreds to thousands of images with variations in lighting, angles, and environments.  

## Data Annotation  
- **Tool Used**: CVAT (Computer Vision Annotation Tool).  
- **Process**: Annotate images by drawing bounding boxes around ducks.  
- **Tips**: Define clear criteria for bounding boxes to ensure consistency and accuracy during training.  

## Preparing the Data  
- **Folder Structure**: Organize data into `training` and `validation` folders.  
- **Naming Conventions**: Use clear and consistent file names to avoid errors.  
- **YOLO Format**:  
  - Class ID  
  - Bounding box coordinates (x, y, width, height, normalized).  

## Training the Model  
- **Setup**: Train locally or on Google Colab for efficiency.  
- **Steps**:  
  1. Upload dataset.  
  2. Install dependencies.  
  3. Configure dataset paths.  
- **Monitoring**: Observe loss functions and accuracy metrics during training.  

## Evaluating the Model  
- **Post-Training Evaluation**:  
  - Analyze loss functions and accuracy to assess model quality.  
  - Compare predictions to ground truth data for insights.  
- **Improvement Suggestions**: Refine the dataset and training parameters iteratively.  

## Making Predictions  
- **Prediction Script**: Apply the trained model on video frames.  
- **Output**: Draw bounding boxes around detected ducks in videos.  
- **Evaluation in Real Scenarios**: Test the model's robustness and identify areas for further optimization.  

--- 
# Day 16: Building an End-to-End Computer Vision Project Pipeline  

## **Introduction**  
- **Focus**: Developing a pipeline for an **image background removal** application.  
- **Pipeline Stages**: Requirements, Planning, Execution, Testing, and Delivery.  
- **Importance**: Ensures efficient, risk-minimized, and client-aligned project delivery.  


## **1. Requirements Gathering**  
- Gather **client specifications**: budget, project description, and outputs.  
- Example: "Build an image background removal model and serve it via an API."  
- **Constraints**:  
  - Single-person images.  
  - Open-source resources preferred to reduce costs.  


## **2. Planning Phase**  
- **State-of-the-Art Review**: Research repositories, papers, and technologies.  
- **Implementation Plan**: Tasks include algorithm setup, API development, and documentation.  
- **Key Tasks**:  
  - Identify existing resources for background removal (e.g., **REM BG**).  
  - Decide on custom or pre-existing algorithms.  
  - Prepare API for model serving.  
  - Plan testing and documentation.  


## **3. Execution Phase**  
- Conduct a **State-of-the-Art Review**.  
- **Implement the Algorithm**: Use repositories like **REM BG** for streamlined development.  
- **Testing the Implementation**: Validate with sample images.  
- **Observations**:  
  - Implementation was faster than estimated.  
  - Risks identified for algorithm adjustments.  


## **4. Testing and Integration**  
- Use tools like **FastAPI** and **NGINX** for API setup.  
- Validate API-model communication and troubleshoot (e.g., S3 bucket errors).  


## **5. Deliverables Preparation**  
- **Deliverables include**:  
  - **API documentation**: Endpoints and usage instructions.  
  - Retraining guidelines for the client.  
- Ensure seamless integration and usability for the client.  


## **Conclusion**  
- **Key Steps**: Requirements, Planning, Execution, Testing, Delivery.  
- **Benefits**: Risk reduction, timely delivery, and high-quality results.  
- **Significance**: A structured pipeline equips professionals with skills to handle complex real-world projects.  


 <img src="https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Images/simplified_pipeline.png" width="1050" height="500"/>

 ---
# Day 17: Building an Image Processing API with AWS  

This project demonstrates how to create a basic image processing API using AWS. It focuses on converting images from BGR to grayscale, guiding through AWS Lambda, S3, and API Gateway integration. The project emphasizes the importance of mastering cloud-based APIs for scalable real-world applications.  

### Key Steps:  
- Understanding AWS components  
- Writing the Python script  
- Setting up the Lambda function  
- Integrating with API Gateway  
- Testing the API  

Here, the API processes images, illustrating how to utilize AWS for computer vision tasks while emphasizing scalability and modularity.  

### Understanding AWS Components  

**Key Services**:  
1. **Lambda Functions**: Serverless computing for backend tasks.  
2. **S3 Buckets**: Storing and retrieving image data.  
3. **API Gateway**: Managing client-server communication.  


### Writing the Python Script  

- **Script Functionality**:  
  - Reads images in Base64 format.  
  - Converts them to grayscale using OpenCV.  
  - Encodes output back to Base64.  

- **Focus**: Test the script locally before deploying.  


### Setting Up the Lambda Function  

**Process**:  
1. **Create Function**: Name it (e.g., "ImageProcessor") and choose Python runtime.  
2. **Add Logic**: Implement decoding, grayscale conversion, and re-encoding.  
3. **Test Locally**: Simulate API input and output.  

**Tips**:  
- Ensure image encoding follows API standards.  
- Address dependencies like OpenCV and NumPy using S3 Lambda layers.  


### Integrating with API Gateway  

1. **Create API**: Define POST method for image processing.  
2. **Link Lambda**: Attach the function to the API.  
3. **Deploy**: Publish the API for external use.  


### Testing the API  

- **Input**: Base64-encoded images.  
- **Output**: Grayscale images returned in Base64 format.  
- **Debugging**: Handle errors (e.g., missing libraries or API misconfigurations).  

### Key Takeaways  
- Mastering Python, OpenCV, and AWS is critical for scalable applications.  
- Building cloud-based APIs enhances developer skill sets for real-world computer vision tasks.  
- Iterative testing ensures API reliability and performance.  


--- 
# Day 18: Evaluating Data Requirements for Training Machine Learning Models (theory)

## Introduction
- Focus: Investigating the data needed to train an **object detection model** using **YOLOv8**.
- Key insight: More data doesn't always mean better performance; efficient data use can reduce costs and maintain accuracy.
- Concepts introduced:
  - **Object Detection**: Identifying objects in images.
  - **YOLOv8**: Real-time object detection algorithm.
  - **Mean Average Precision (mAP)**: Accuracy metric for object detection.
  - **Training Epochs**: Complete passes through the training dataset.

## Understanding the Experiment
- Experiment involves training a model with datasets of varying sizes: 10, 50, 100, 200, 500, 1,000, 2,000, and 4,000 images.
- Objectives:
  - Assess performance (mAP) and efficiency against dataset sizes.
  - Identify the minimal dataset size for satisfactory accuracy.

## Experimental Setup
- Data sampled from Google Open Images Dataset Version 7.
- Consistent training methodology:
  - **20 epochs** for training each model.
  - Fixed test set of **100 images** for performance evaluation.

## Analysis of Results
### Performance Metrics
1. **mAP vs. Dataset Size**:
   - mAP increases with dataset size: **60% (10 images)** to **91.1% (4,000 images)**.
   - Marginal improvements with larger datasets.
2. **Training Time vs. Dataset Size**:
   - Training time grows exponentially with dataset size.
   - Training with 4,000 images is 7x slower than with 500 images, for only a **0.05% mAP** gain.

### Key Observations
- **Diminishing Returns**: Accuracy gains decrease as dataset size increases.
- **Practical Constraints**: Time and resource costs must be balanced against performance benefits.

## Real-World Implications
- Models tested on videos of ducks in various scenarios:
  - Models trained with fewer images (10â€“50) failed.
  - Models trained with 100 images showed errors.
  - Models with 2,000â€“4,000 images performed best.

### Video Analysis Insights
- Metrics like mAP can be misleading without real-world validation.
- A model with **73% mAP** may still fail in practical scenarios.

## Conclusion
### Main Takeaways
- **Optimal Data Usage**: Balance dataset size and accuracy to minimize effort and cost.
- **Efficiency Matters**: Training resources must justify performance improvements.
- **Metrics vs. Reality**: Validate models in real-world settings to align statistics with practical outcomes.

--- 
# Day 19:














