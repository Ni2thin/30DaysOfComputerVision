# 30 Days of Computer Vision 🚀

![Last Commit](https://img.shields.io/github/last-commit/Ni2thin/30DaysOfComputerVision)
![Repo Size](https://img.shields.io/github/repo-size/Ni2thin/30DaysOfComputerVision)

Like the rising sun, every new day brings an opportunity to shine brighter. With patience, determination, and resilience, we will paint our path in the world of computer vision. **七転び八起き** (Nanakorobi yaoki) – "Fall seven times, stand up eight."



## Resources and Progress 📚

| Books & Resources                                   | Completion Status | 
|-----------------------------------------------------|-------------------|
| [Machine Learning Specialization on Coursera](https://www.coursera.org/specializations/machine-learning-introduction) | 📈        |      
| [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) |   📈   |      
| [Computer Vision YouTube Playlist](https://www.youtube.com/watch?v=HiTw5KFw7ic&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd) |  ![Playlist Thumbnail](https://img.youtube.com/vi/HiTw5KFw7ic/hqdefault.jpg) |

## Progress Tracker

| Day  | Date         | Topics                                               | Resources                                          |
|------|--------------|------------------------------------------------------|----------------------------------------------------|
| Day 1 | 27-12-2024  | OpenCV tutorial for beginners                        | [OpenCV tutorial](https://www.youtube.com/watch?v=eDIj5LuIL4A&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=2)     |
| Day 2 | 28-12-2024  | Detecting color with Python and OpenCV               | [Detecting color](https://www.youtube.com/watch?v=aFNDh5k3SjU&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=4)                   |
| Day 3 | 29-12-2024  | Face detection and blurring with Python              | [Face detection and blurring](https://www.youtube.com/watch?v=DRMBqhrfxXg&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=4)       |
| Day 4 | 30-12-2024  | Text detection with Python(Tesseract)             | [Text detection with Python](https://www.youtube.com/watch?v=CcC3h0waQ6I&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=5)        |
| Day 5 | 31-12-2024  | Image classification with Python and OpenCV          | [Image classification with Python](https://www.youtube.com/watch?v=il8dMDlXrIE&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=6&pp=iAQB)  |
| Day 6 | 01-01-2025  | Emotion detection with Python, OpenCV, and others    | [Emotion detection](https://www.youtube.com/watch?v=h0LoewzGzhc&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=8)                 |
| Day 7 | 02-01-2025  | Image classification + feature extraction           | [Image classification + feature extraction](https://youtu.be/oEKg_jiV1Ng?feature=shared) |
| Day 8 | 03-01-2025  | Sign language detection with Python and OpenCV       | [Sign language detection](https://youtu.be/MJCSjXepaAM?feature=shared)           |
| Day 9 | 04-01-2025  | Image classification WEB APP with Python & Flask     | [Image classification web app](https://youtu.be/n_eMARPqBZI?feature=shared)      |
| Day 10 | 05-01-2025 | AWS Rekognition tutorial (Object detection)      | [AWS Rekognition](https://www.youtube.com/watch?v=C9H_v44670s)                   |
| Day 11 | 06-01-2025 | Yolov8 object tracking 100% native                   | [Yolov8 object tracking](https://youtu.be/uMzOcCNKr5A?feature=shared)            |
| Day 12 | 07-01-2025 | Image segmentation with Yolov8 and Segment Anything Model(SAM)  | [Image segmentation with Yolov8](https://www.youtube.com/watch?v=aYP4ujUsGdk)    |
| Day 13 | 08-01-2025 | Pose detection using webcam      | [Train pose detection ](https://www.youtube.com/watch?v=06TE_U21FK4)       |
| Day 14 | 09-01-2025 | Parking spot detection and counter                   | [Parking spot detection](https://www.youtube.com/watch?v=F-884J2mnOY&list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd&index=15&pp=iAQB)            |
| Day 15 | 10-01-2025 | Train Yolov10 object detection custom dataset        | [Train Yolov10 object detection](https://www.youtube.com/watch?v=PfQwNe0P-G4)    |
| Day 16 | 11-01-2025 | End to end pipeline real world computer vision       | [End to end pipeline](https://youtu.be/xgtujvjKIGs?feature=shared)               |
| Day 17 | 12-01-2025 | Image processing API with AWS API Gateway            | [Image processing API](https://www.youtube.com/watch?v=iUoiBRwyIBg)              |
| Day 18 | 13-01-2025 | How much data you need to train a computer vision model | [Theoritical evaluation](https://youtu.be/8YXk_zcllC8?feature=shared)                 |
| Day 19 | 14-01-2025 | Real world application of computer vision           | [Invoice logo detection](https://youtu.be/OP8AozaEuLM?feature=shared)            |
| Day 20 | 15-01-2025 | Train Detectron2 object detection custom dataset     | [Train Detectron2](https://youtu.be/I7O4ymSDcGw?feature=shared)                  |
| Day 21 | 16-01-2025 | Face recognition on your webcam with Python & OpenCV | [Face-Api.js](https://www.youtube.com/watch?v=yBgXx0FLYKc)                  |
| Day 22 | 17-01-2025 | Face attendance + face recognition with OpenCV      | [Face attendance](https://www.youtube.com/watch?v=z_dbnYHAQYg)                   |
| Day 23 | 18-01-2025 | Machine learning with AWS practical projects        | [ML with AWS](https://youtu.be/ZvBZh8ky-zQ?feature=shared)                       |
| Day 24 | 19-01-2025 | Chat with an image (LangChain custom)                | [Chat with an image](https://youtu.be/71EOM5__vkI?feature=shared)                |
| Day 25 | 20-01-2025 | Image generation with Python (Train Stable Diffusion) | [Image generation](https://youtu.be/_yjNLzUwFhA?feature=shared)                  |
| Day 26 | 21-01-2025 | Real-Time Face Recognition and Attendance System              | [Face attendance with GUI](https://www.youtube.com/watch?v=rVQv1TwvQBc&list=PL6DmgjJvD9OafqxqHwPYl3y-hVNBmfNnc)       |
| Day 27 | 22-01-2025 | Face recognition and face matching with Python      | [Face matching](https://youtu.be/FavHtxgP4l4?feature=shared)                     |
| Day 28 | 23-01-2025 | Machine learning web app with Python, Flask, and others | [ML web app](link)                     |
| Day 29 | 24-01-2025 | Object detection on Raspberry Pi USB camera         | [Object detection on Raspberry Pi](link)  |
| Day 30 | 25-01-2025 | Image generation with Python & Stable Diffusion     | [Image generation](link)                  |

---

# Day 1: Getting started with OpenCV

## Key Concepts

- **Images in CV2**: Images are stored as **NumPy arrays** in OpenCV.
- **Image Shape**: The shape of an image consists of **height, width, and channels** (BGR format).
- **Color Format**: 
  - OpenCV uses **BGR** (Blue, Green, Red) while libraries like Matplotlib use **RGB**.
  - To convert from BGR to RGB, use:
    ```
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    ```
- [Introduction to CV2 ](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/CV2%20Intro)   

## Image Manipulation

- **Shape and Resize**: Use `cv2.resize()` to change the dimensions of an image.
- **Cropping**: Crop images by slicing the NumPy array.

## Color Space Conversions

- Convert between color spaces using functions like:
  - `cv2.cvtColor(image, cv2.COLOR_BGR2RGB)` for BGR to RGB
  - `cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)` for BGR to Grayscale
  - `cv2.cvtColor(image, cv2.COLOR_BGR2HSV)` for BGR to HSV

## Blurring Techniques

- Use various blurring methods such as:
  - `cv2.blur()`
  - `cv2.medianBlur()`
  - `cv2.GaussianBlur()`

## Thresholding Methods

- Apply different thresholding techniques for segmentation:
  - **Global Thresholding**
  - **Adaptive Thresholding**
  - Converts images from normal to binary.

## Edge Detection Techniques

- Utilize edge detection algorithms like:
  - `cv2.Sobel()`
  - `cv2.Canny()`
  - `cv2.Laplacian()`
- Morphological operations such as dilation and erosion can also be applied.

## Drawing Functions

- Draw shapes on images using functions like:
  - `cv2.line()`
  - `cv2.rectangle()`
  - `cv2.circle()`

## Contours

### Features:
- **Grayscale Conversion**: Converts the input image to grayscale for easier processing.
- **Thresholding**: Applies binary inverse thresholding to create a binary image suitable for contour detection.
- **Contour Extraction**: Uses OpenCV's `findContours` to extract object contours from the thresholded image.
- **Bounding Boxes**: Calculates and draws bounding rectangles around contours exceeding a specified area threshold.
- **Dynamic Visualization**: Displays the original image with bounding boxes and the thresholded binary image for analysis.

  ---
  
# Day 2: Detecting color with Python and OpenCV 

This project uses OpenCV to detect and track objects of a specific color in real-time using a webcam feed. It processes frames to isolate the target color, highlights detected objects with bounding boxes, and displays the result. [color detection](https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Projects/color-detection.py)

## Features

- Dynamic HSV Range:
Adapts HSV thresholds for any BGR color, useful for varying lighting conditions.
- Hue Wrapping for Red:
Handles HSV hue wrapping (0–10 and 170–180) to avoid missing red objects.
- Noise Reduction:
Combines GaussianBlur and morphological operations (MORPH_CLOSE, MORPH_OPEN) for clean masks.
- Real-Time Optimization:
Ignores small noise with contour area thresholding (area > 500).
- Mirror-Like View:
Flips frames (cv2.flip) for intuitive user interaction.
- Dynamic Highlighting:
Draws bounding boxes around detected objects; replace with contours for precise outlines.
- Fail-Safe Video Capture:
Handles webcam failures gracefully with if not ret.
- Multi-Color Support:
Extendable to detect multiple colors by blending masks.

---

# Day 3: Face Anonymizer  

This project implements face detection and blurring using **OpenCV** and **Mediapipe**. The application supports three different modes for processing images and video.
[Face Anonymizer output](Images/face-blur.png)

### Key Features:
- **Face Detection Model**: Uses Mediapipe's face detection model with a **50% confidence threshold** to detect faces.
- **Blurring**: A Gaussian blur with a kernel size of **(30, 30)** is applied to the detected faces.
- **Video Processing**: Processes video frames at **25 FPS** and saves them in an output directory.
- **Output**: All processed images and videos are saved in the **output** directory.
  
---

# Day 4: Text detector using tesseract

## **Features**
1. **Text Detection in Images**
   - Processes an input image to detect and extract text.
   - Highlights detected text regions with bounding boxes.
   
2. **Real-Time Text Detection via Webcam**
   - Captures webcam feed to detect and extract text in real-time.
   - Displays processed frames with detected text.


## **Key Concepts**
1. **Preprocessing Techniques**
   - Convert images to grayscale.
   - Apply thresholding to enhance text regions.
   - Use morphological operations (dilation) to emphasize text.

2. **Text Detection**
   - Use Tesseract OCR with custom configurations:
     - **OEM 3**: Tesseract engine mode for both legacy and LSTM models.
     - **PSM 6**: Page segmentation mode for detecting text blocks.
   - Extract text data (including bounding box coordinates) using `pytesseract.image_to_data`.

3. **Bounding Boxes**
   - Draw rectangles around detected text regions with confidence > 50%.
   
---

# Day 5: Parking Spot Detection and Classification

This project leverages Python and OpenCV to detect and classify parking spots in real-time. It processes video feeds or images to identify available and occupied parking spots,employing advanced image processing techniques to provide efficient parking management solutions. [Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/image-classification)

## **Features**
1. **Parking Spot Detection**  
   - Detect parking spots in video using a binary mask and extract regions of interest (ROIs).

2. **Real-Time Spot Classification**  
   - Classify each spot as `empty` or `not_empty` using a pre-trained SVM model.  
   - Update status dynamically and visualize with:
     - **Green** bounding boxes for `empty` spots.  
     - **Red** bounding boxes for `not_empty` spots.

3. **Availability Counter**  
   - Displays the number of available spots in real-time on the video feed.

4. **Efficient Frame Processing**  
   - Analyze frames at intervals to optimize performance while ensuring accuracy.

### **Output**
<img src="https://github.com/Ni2thin/30DaysOfComputerVision/blob/04c4f59c6f005878688dd18dac90a65fe234c853/Projects/image-classification/Parking%20spot%20detection/parking%20spot%20-%20output.png" width="1050" height="500"/>

### **Assets** 
[Download Dataset](https://drive.google.com/drive/folders/15lLq-6Bbuq7LyILMg2rzJOL4WpnxX6oX?usp=share_link) 

---
# Day 6: Emotion Detection with Python and OpenCV  

This project utilizes Python, OpenCV, and face landmark detection to classify human emotions from images. It processes face images, extracts 1404 key landmarks, and organizes them into labeled datasets for emotion classification. [Emotion detection project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Emotion%20detection)  

## **Features**  

- **Emotion Categorization**:  
  Processes images into seven predefined emotional categories (e.g., happy, sad, angry).  

- **Facial Landmark Extraction**:  
  Uses a custom `get_face_landmarks` function to capture precise facial landmarks essential for emotion detection.  

- **Batch Processing**:  
  Efficiently processes images in batches to optimize resource usage and avoid system overloads.  

- **Intermediate Data Storage**:  
  Saves labeled datasets for each emotion in separate text files (`data_<emotion>.txt`) and combines them into a single dataset (`data.txt`).  

- **Robust File Handling**:  
  Skips invalid or non-image files to ensure smooth processing.  

- **Scalable Architecture**:  
  Easily extendable to support more emotions or larger datasets without requiring significant code changes.  

- **Threading Optimization**:  
  Limits system thread usage to prevent crashes and improve stability, especially on macOS.  

---
# Day 7: Image Classification and Feature Extraction with Python  

This project leverages `Img2Vec` and a pre-trained Random Forest model to classify images into predefined categories. It extracts feature vectors from images, predicts their labels, and provides confidence scores for each classification. [Image classification and feature extraction project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/img%20classification%20and%20extraction)

## **Features**  

- **Image Feature Extraction**:  
  Utilizes `Img2Vec` to generate 512-dimensional feature vectors, representing high-level characteristics of images.  

- **Robust Classification**:  
  Employs a Random Forest classifier to predict image labels with high accuracy.  

- **Confidence Scoring**:  
  Outputs confidence scores alongside predictions to gauge the reliability of the results.  

- **Error Handling**:  
  Includes checks for file existence and compatibility to prevent crashes during runtime.  

- **Human-Readable Output**:  
  Converts predictions into category labels (e.g., "Sunny", "Cloudy") for better interpretability.  

- **Flexible Input**:  
  Compatible with various image formats (e.g., JPG, PNG, BMP).  

- **Modular Workflow**:  
  Easily extendable to add more categories or integrate advanced models for improved classification.  

---
# Day 8: Building a Sign Language Detector with Python and OpenCV

This project implements a real-time sign language detection system using OpenCV, MediaPipe, and a machine learning model. The system captures hand gestures via a webcam, processes them to extract features, and predicts corresponding sign language characters. 
[Sign Language Detection Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Sign%20language%20detection)


## Features

- **Dataset Creation**: Captures 100 images per class using the webcam for generating a labeled dataset.
- **Feature Extraction**: Utilizes MediaPipe's hand landmarks to extract normalized hand features (x, y coordinates).
- **Model Training**: Trains a Random Forest Classifier on the extracted features to distinguish between gestures.
- **Real-Time Prediction**: Detects hand gestures in live webcam feed and overlays the predicted character on the video stream.
- **Visualization**: Draws hand landmarks and bounding boxes around detected hands for intuitive visualization.
- **Expandable Design**: Easy to extend with additional gestures or classes by adding new labeled data.

## Workflow

### 1. Data Collection
- Webcam feed captures images for each gesture class.
- Images are saved to class-specific directories.

### 2. Model Training
- Extracts hand landmarks and normalizes them.
- Trains a Random Forest model on the features for classification.

### 3. Real-Time Detection
- Processes live webcam feed to detect and classify gestures.
- Displays predictions with bounding boxes and labels.

## Key Innovations

- **Hand Landmarks**: MediaPipe's hand tracking ensures accurate gesture representation.
- **Efficient Dataset Handling**: Automates data collection and preprocessing for rapid model training.
- **Dynamic Prediction**: Real-time responsiveness allows seamless interaction with the system.
- **Scalable Architecture**: Can be extended to support more complex sign language alphabets.

## How to Use

1. **Dataset Creation**:
   - Run the data collection script to capture hand gesture images.
   - Ensure each gesture has its own labeled folder in the dataset directory.

2. **Model Training**:
   - Run the training script to train the Random Forest Classifier.
   - A trained model will be saved for later use.

3. **Real-Time Detection**:
   - Launch the detection script to start the webcam feed.
   - The system will predict and display the corresponding gesture in real-time.

## Example Output

The system detects gestures and overlays predictions directly on the live video feed:
<p align="center">
  <img src="Projects/Sign%20language%20detection/outputs/B.png" alt="Image 1" height="300px">
  <img src="Projects/Sign%20language%20detection/outputs/A.png" alt="Image 2" height="300px">
</p>

---
# Day 9: Pneumonia Classification - Simple web app

This project utilizes Python and Keras to classify chest X-ray images as either showing signs of pneumonia or being normal. The model is trained using the Teachable Machine platform, and the dataset is taken from Kaggle's Chest X-Ray dataset. The goal is to assist in the identification of pneumonia cases using deep learning. [Project](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Pneumonia%20classification)

## **Features**
1. **Pneumonia Detection**  
   - Classify chest X-ray images into two categories: `Pneumonia` and `Normal`.
   - Use pre-trained models fine-tuned with a custom dataset to provide accurate results.

2. **Prediction Confidence**  
   - Displays the model's confidence in the prediction as a percentage.
   - Provide a progress bar for an interactive user experience.

3. **Real-Time Upload and Prediction**  
   - Upload a chest X-ray image and get an immediate prediction for pneumonia classification.
   - Show prediction results on the app with a confidence score.

4. **User-Friendly Interface**  
   - The web app is designed with a simple and intuitive UI, allowing users to easily upload and classify chest X-ray images.
   - Streamlit is used to build an interactive and seamless user experience.

### **Output**
<p align="center">
  <img src="Projects/Pneumonia classification/output/output.png" alt="Image 1" height="300px">
  <img src="Projects/Pneumonia classification/output/output2.png" alt="Image 2" height="300px">
</p>

### **Assets**
- [Download Dataset from Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data)
- [Teachable Machine](https://teachablemachine.withgoogle.com/)
---
# Day 10: Object detection using AWS Rekognition 

This project demonstrates the use of AWS Rekognition to detect objects in a video stream. Specifically, it identifies instances of a target class (`Zebra` in this example) and saves the bounding box annotations and corresponding frames. The implementation leverages AWS Rekognition's label detection capabilities to process video frames and extract object information.

## **Features**
1. **Frame Processing and Object Detection**
   - Reads video frames using OpenCV.
   - Sends each frame to AWS Rekognition for object detection.
   - Identifies and processes instances of the target class (`Zebra`).

2. **Bounding Box Annotation**
   - Extracts bounding box details (center coordinates, width, and height) for the detected objects.
   - Saves annotations in YOLO-compatible format as text files for each frame.

3. **Frame Output Storage**
   - Saves each processed video frame as an image in a designated directory.
   - Frames are named sequentially for easy reference.

4. **Customizable Target Class**
   - Allows users to specify a different target object class by modifying the `target_class` variable.

5. **AWS Rekognition Integration**
   - Utilizes AWS Rekognition’s `detect_labels` API for high-confidence object detection.
   - Customizable minimum confidence threshold for detections.

## **Usage**
### Prerequisites
1. AWS Rekognition credentials:
   - Ensure you have an active AWS account with access to Rekognition services.
   - Replace the placeholders in `credentials.py` with your AWS Access Key and Secret Key:
     ```python
     access_key = "YOUR_AWS_ACCESS_KEY"
     secret_key = "YOUR_AWS_SECRET_KEY"
     ```

2. Install the required Python libraries:
   ```bash
   pip install boto3 opencv-python
   ```
   --- 
# Day 11: Object Detection with YOLO and OpenCV

This project demonstrates how to use the YOLO (You Only Look Once) model from the Ultralytics library to perform real-time object detection on a video file. It processes video frames, applies YOLO for object detection, and displays the results with bounding boxes and annotations. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Object%20detection%20using%20YOLO)

## Features

- **Pretrained YOLO Models**: Utilizes the YOLOv8 model (`yolov8n.pt`) for lightweight and fast object detection.  
- **Video File Input**: Processes video frames from a local file (`cat.mp4`).  
- **Real-Time Detection**: Performs real-time object detection on each frame with high accuracy.  
- **Dynamic Visualization**: Displays detected objects with bounding boxes and labels overlaid on the video.  
- **Customizable Models**: Replace `yolov8n.pt` with other YOLO weights (`yolov8s.pt`, `yolov8m.pt`, etc.) for higher precision.  
- **Fail-Safe Video Handling**: Checks and handles errors if the video file cannot be opened or frames cannot be read.  
- **Interactive Interface**: Press `q` to exit the visualization during playback.

## Prerequisites

- Python 3.8 or later
- Install required libraries:
  ```bash
  pip install ultralytics opencv-python
  ```
---
# Day 12: Image segmentation with Yolov8 and Segment Anything Model(SAM) 

This project demonstrates how to detect and segment objects in an image using **YoloV8** for object detection, **Segment Anything Model (SAM)** for segmentation, and finally removing the background. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Image%20segmentation%20using%20yolov8%20and%20SAM)

## Steps

### 1. Install Dependencies

Install YoloV8 and SAM, and download the SAM model checkpoint.
```bash
!pip install ultralytics
!pip install 'git+https://github.com/facebookresearch/segment-anything.git'
!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
```
### 2. Object Detection with YoloV8

Detect objects in the image.

```bash
!yolo predict model=yolov8n.pt source='your-image.jpeg'
```
- visualize detection
  ``` python
  Image(filename='/content/runs/detect/predict/your-image.jpg', height=600)
  ```
### 3. Segmentation with SAM

Use SAM to segment objects based on YoloV8 bounding boxes.
Visualize segmentation results:

```python
masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)
plt.imshow(image)
show_mask(masks[0], plt.gca())
plt.show()
```
### 4. Background Removal

Create a binary mask and isolate the object:

``` python
binary_mask = np.where(segmentation_mask > 0.5, 1, 0)
new_image = (255 * (1 - binary_mask[..., None]) + image * binary_mask[..., None])
plt.imshow(new_image.astype(np.uint8))
plt.axis('off')
plt.show()
```
---
# Day 13: Pose Detection and Rep Counter

## Overview
This project utilizes **OpenCV** and **MediaPipe** to perform real-time pose detection and calculate the angles of arm movements for fitness tracking. The system detects arm curls, tracks the number of repetitions for each arm, and provides a live visual feed with overlaid information. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/AI%20Pose%20Estimation%20for%20gym)

## Features
- **Real-Time Pose Detection:** Detects human poses using MediaPipe's Pose solution.
- **Angle Calculation:** Calculates joint angles to determine arm movement stages (e.g., "up" or "down").
- **Repetition Counter:** Tracks and counts curls for both left and right arms.
- **Camera Flipping:** Mirrors the video feed for a natural user experience.
- **On-Screen Visuals:**
  - Real-time joint angles.
  - Left and right arm repetition counts.
  - Frames Per Second (FPS).
  - Instructions for pausing or quitting.
- **Data Logging:** Saves the final repetition counts to a text file (`reps_data.txt`).

## Curl Logic
The repetition counter uses the calculated angle of the elbow joint to determine the arm's movement stages ("up" or "down") and to count completed curls.

1. **Landmark Detection:**
   - Extract the coordinates of the shoulder, elbow, and wrist for each arm using MediaPipe landmarks.

2. **Angle Calculation:**
   - Calculate the angle at the elbow joint using the formula:
      angle = | atan2(c_y - b_y, c_x - b_x) - atan2(a_y - b_y, a_x - b_x) | × 180/π

     where:
     - `a`, `b`, and `c` are the shoulder, elbow, and wrist coordinates, respectively.

3. **Movement Stages:**
   - If the angle exceeds 160°, the arm is considered **"down"** (extended).
   - If the angle drops below 30° and the arm was previously "down," the arm is considered **"up"** (flexed).

4. **Increment Counter:**
   - Transitioning from "down" to "up" increments the repetition counter.

## Instructions
1. Run the script.
2. Perform arm curls in front of the camera.
3. View real-time feedback, including repetitions and angles.
4. Press `P` to pause or `Q` to quit.
5. Check the `reps_data.txt` file for a summary of your session.


## Use Cases
- Fitness tracking for arm workouts.
- Pose analysis for personal or professional fitness training.
- Real-time feedback for improving exercise form.

This project provides a foundation for fitness applications with advanced pose estimation features.

---
# Day 14: Parking Spot Detection System

A computer vision-based solution to detect parking spot availability, designed for versatile use cases, including Indian parking scenarios [link.](https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Projects/image-classification/Parking%20spot%20detection/improved-parking-spot-detection.py)

## Features

- **Dynamic Configuration**: Use command-line arguments for mask and video paths.
- **Optimized Processing**: Configurable frame skipping (`--step`) and sensitivity (`--diff_threshold`).
- **Indian Context Support**: Handles compact spaces, irregular layouts, and lighting variations.
- **Real-Time Visualization**: Color-coded bounding boxes for parking spots.
- **Modular Design**: Easily adaptable for live feeds or IoT integration.

## Installation

Clone the repository:
   ```bash
   git clone https://github.com/Ni2thin/30DaysOfComputerVision.git
   cd Projects/image-classification/Parking spot detection/improved-parking-spot-detection.py
```
## Command-Line Arguments

| Argument          | Description                                    | Default    |
|--------------------|------------------------------------------------|------------|
| `--mask`          | Path to the binary mask image.                | Required   |
| `--video`         | Path to the video file.                       | Required   |
| `--step`          | Frame processing interval.                    | `30`       |
| `--diff_threshold`| Threshold for detecting spot changes.         | `0.4`      |

## Output

- **Visualization**: 
  - Green boxes for available spots, red for occupied.
- **Console Logs**: Spot change metrics and parking availability.

## Limitations

- **Lighting Sensitivity**: Adjust `--diff_threshold` for shadows or glare.
- **Mask-Video Alignment**: Ensure perfect alignment for accurate detection.
- **Camera Angle**: Best results with top-down or high-angle views.
---
# Day 15: Object detection using YOLOv10 with custom data

 This project demonstrates how to build a custom object detector using YOLOv10, from dataset creation to deploying a functional model. It emphasizes the iterative nature of training and evaluation to achieve reliable real-world performance. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Object%20detection%20using%20YOLOv10)

## key steps:  
- Data collection  
- Data annotation  
- Preparing the dataset  
- Training the model  
- Evaluating the model  
- Making predictions  

 Here the dataset focuses on images of ducks, demonstrating the importance of a structured approach to train an effective object detection model.

## Data Collection  
- **Dataset Description**: Images of ducks in various sizes, colors, and backgrounds.  
- **Importance**: A large and diverse dataset enhances the model's generalization capabilities.  
- **Recommendations**: Collect hundreds to thousands of images with variations in lighting, angles, and environments.  

## Data Annotation  
- **Tool Used**: CVAT (Computer Vision Annotation Tool).  
- **Process**: Annotate images by drawing bounding boxes around ducks.  
- **Tips**: Define clear criteria for bounding boxes to ensure consistency and accuracy during training.  

## Preparing the Data  
- **Folder Structure**: Organize data into `training` and `validation` folders.  
- **Naming Conventions**: Use clear and consistent file names to avoid errors.  
- **YOLO Format**:  
  - Class ID  
  - Bounding box coordinates (x, y, width, height, normalized).  

## Training the Model  
- **Setup**: Train locally or on Google Colab for efficiency.  
- **Steps**:  
  1. Upload dataset.  
  2. Install dependencies.  
  3. Configure dataset paths.  
- **Monitoring**: Observe loss functions and accuracy metrics during training.  

## Evaluating the Model  
- **Post-Training Evaluation**:  
  - Analyze loss functions and accuracy to assess model quality.  
  - Compare predictions to ground truth data for insights.  
- **Improvement Suggestions**: Refine the dataset and training parameters iteratively.  

## Making Predictions  
- **Prediction Script**: Apply the trained model on video frames.  
- **Output**: Draw bounding boxes around detected ducks in videos.  
- **Evaluation in Real Scenarios**: Test the model's robustness and identify areas for further optimization.  

--- 
# Day 16: Building an End-to-End Computer Vision Project Pipeline  

## **Introduction**  
- **Focus**: Developing a pipeline for an **image background removal** application.  
- **Pipeline Stages**: Requirements, Planning, Execution, Testing, and Delivery.  
- **Importance**: Ensures efficient, risk-minimized, and client-aligned project delivery.  


## **1. Requirements Gathering**  
- Gather **client specifications**: budget, project description, and outputs.  
- Example: "Build an image background removal model and serve it via an API."  
- **Constraints**:  
  - Single-person images.  
  - Open-source resources preferred to reduce costs.  


## **2. Planning Phase**  
- **State-of-the-Art Review**: Research repositories, papers, and technologies.  
- **Implementation Plan**: Tasks include algorithm setup, API development, and documentation.  
- **Key Tasks**:  
  - Identify existing resources for background removal (e.g., **REM BG**).  
  - Decide on custom or pre-existing algorithms.  
  - Prepare API for model serving.  
  - Plan testing and documentation.  


## **3. Execution Phase**  
- Conduct a **State-of-the-Art Review**.  
- **Implement the Algorithm**: Use repositories like **REM BG** for streamlined development.  
- **Testing the Implementation**: Validate with sample images.  
- **Observations**:  
  - Implementation was faster than estimated.  
  - Risks identified for algorithm adjustments.  


## **4. Testing and Integration**  
- Use tools like **FastAPI** and **NGINX** for API setup.  
- Validate API-model communication and troubleshoot (e.g., S3 bucket errors).  


## **5. Deliverables Preparation**  
- **Deliverables include**:  
  - **API documentation**: Endpoints and usage instructions.  
  - Retraining guidelines for the client.  
- Ensure seamless integration and usability for the client.  


## **Conclusion**  
- **Key Steps**: Requirements, Planning, Execution, Testing, Delivery.  
- **Benefits**: Risk reduction, timely delivery, and high-quality results.  
- **Significance**: A structured pipeline equips professionals with skills to handle complex real-world projects.  


 <img src="https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Images/simplified_pipeline.png" width="1050" height="500"/>

 ---
# Day 17: Building an Image Processing API with AWS  

This project demonstrates how to create a basic image processing API using AWS. It focuses on converting images from BGR to grayscale, guiding through AWS Lambda, S3, and API Gateway integration. The project emphasizes the importance of mastering cloud-based APIs for scalable real-world applications.  

### Key Steps:  
- Understanding AWS components  
- Writing the Python script  
- Setting up the Lambda function  
- Integrating with API Gateway  
- Testing the API  

Here, the API processes images, illustrating how to utilize AWS for computer vision tasks while emphasizing scalability and modularity.  

### Understanding AWS Components  

**Key Services**:  
1. **Lambda Functions**: Serverless computing for backend tasks.  
2. **S3 Buckets**: Storing and retrieving image data.  
3. **API Gateway**: Managing client-server communication.  


### Writing the Python Script  

- **Script Functionality**:  
  - Reads images in Base64 format.  
  - Converts them to grayscale using OpenCV.  
  - Encodes output back to Base64.  

- **Focus**: Test the script locally before deploying.  


### Setting Up the Lambda Function  

**Process**:  
1. **Create Function**: Name it (e.g., "ImageProcessor") and choose Python runtime.  
2. **Add Logic**: Implement decoding, grayscale conversion, and re-encoding.  
3. **Test Locally**: Simulate API input and output.  

**Tips**:  
- Ensure image encoding follows API standards.  
- Address dependencies like OpenCV and NumPy using S3 Lambda layers.  


### Integrating with API Gateway  

1. **Create API**: Define POST method for image processing.  
2. **Link Lambda**: Attach the function to the API.  
3. **Deploy**: Publish the API for external use.  


### Testing the API  

- **Input**: Base64-encoded images.  
- **Output**: Grayscale images returned in Base64 format.  
- **Debugging**: Handle errors (e.g., missing libraries or API misconfigurations).  

### Key Takeaways  
- Mastering Python, OpenCV, and AWS is critical for scalable applications.  
- Building cloud-based APIs enhances developer skill sets for real-world computer vision tasks.  
- Iterative testing ensures API reliability and performance.  


--- 
# Day 18: Evaluating Data Requirements for Training Machine Learning Models (theory)

## Introduction
- Focus: Investigating the data needed to train an **object detection model** using **YOLOv8**.
- Key insight: More data doesn't always mean better performance; efficient data use can reduce costs and maintain accuracy.
- Concepts introduced:
  - **Object Detection**: Identifying objects in images.
  - **YOLOv8**: Real-time object detection algorithm.
  - **Mean Average Precision (mAP)**: Accuracy metric for object detection.
  - **Training Epochs**: Complete passes through the training dataset.

## Understanding the Experiment
- Experiment involves training a model with datasets of varying sizes: 10, 50, 100, 200, 500, 1,000, 2,000, and 4,000 images.
- Objectives:
  - Assess performance (mAP) and efficiency against dataset sizes.
  - Identify the minimal dataset size for satisfactory accuracy.

## Experimental Setup
- Data sampled from Google Open Images Dataset Version 7.
- Consistent training methodology:
  - **20 epochs** for training each model.
  - Fixed test set of **100 images** for performance evaluation.

## Analysis of Results
### Performance Metrics
1. **mAP vs. Dataset Size**:
   - mAP increases with dataset size: **60% (10 images)** to **91.1% (4,000 images)**.
   - Marginal improvements with larger datasets.
2. **Training Time vs. Dataset Size**:
   - Training time grows exponentially with dataset size.
   - Training with 4,000 images is 7x slower than with 500 images, for only a **0.05% mAP** gain.

### Key Observations
- **Diminishing Returns**: Accuracy gains decrease as dataset size increases.
- **Practical Constraints**: Time and resource costs must be balanced against performance benefits.

## Real-World Implications
- Models tested on videos of ducks in various scenarios:
  - Models trained with fewer images (10–50) failed.
  - Models trained with 100 images showed errors.
  - Models with 2,000–4,000 images performed best.

### Video Analysis Insights
- Metrics like mAP can be misleading without real-world validation.
- A model with **73% mAP** may still fail in practical scenarios.

## Conclusion
### Main Takeaways
- **Optimal Data Usage**: Balance dataset size and accuracy to minimize effort and cost.
- **Efficiency Matters**: Training resources must justify performance improvements.
- **Metrics vs. Reality**: Validate models in real-world settings to align statistics with practical outcomes.

--- 
# Day 19: Invoice Logo Detection with YOLO
 This project involves Automating invoice logo detection using YOLOv5, featuring synthetic dataset generation with robust annotation and evaluation workflows.
  [link.](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Invoice%20logo%20detection)

## Features
- **Synthetic Dataset Creation**:
  - Dynamically places logos onto background images with randomized scaling and positioning.
  - Ensures diversity by applying random transformations to logos.
- **YOLO-Compatible Annotations**:
  - Generates bounding box labels in YOLO format for seamless integration.
- **Model Evaluation**:
  - Tests the trained model on both synthetic and real-world datasets.

### Workflow
1. **Synthetic Dataset Generation**:
   - Logos are resized and placed randomly on background images.
   - Outputs include annotated images and YOLO-compatible `.txt` files.
2. **Model Training**:
   - Custom YOLO model is trained using the generated synthetic dataset.
   - Optimized for object detection performance.
3. **Testing and Evaluation**:
   - Evaluates detection accuracy on test images.
   - Visualizes predictions with bounding boxes and confidence scores.

### Key Technical Highlights
- **Dynamic Image Synthesis**:
  - Randomized scaling (25%-50% of the background-size) ensures diverse training samples.
  - Alpha channel handling for logo transparency.
- **YOLO Integration**:
  - Bounding boxes normalized for YOLO compatibility (center_x, center_y, width, height).
- **Scalability**:
  - Supports adding new logos and backgrounds with minimal configuration.

### How to Use
1. **Generate Synthetic Dataset**:
   - Run the dataset creation script to produce labelled images and annotations.
2. **Train the YOLO Model**:
   - Use the synthetic dataset to train a YOLO model.
   - Save the trained weights for inference.
3. **Perform Logo Detection**:
   - Use the trained model to detect logos in unseen images.
   - Visualize bounding boxes and labels directly on images.

### Visualization
- Bounding boxes highlight detected logos with class labels for clarity.
- Confidence scores aid in assessing detection reliability.

### **Output**
<p align="center">
  <img src="Projects/Invoice logo detection/predictions/invoice data1.png" alt="Image 1" width="400px" height="300px">
  <img src="Projects/Invoice logo detection/predictions/P_curve.png" alt="Image 2" width="400px" height="300px">
</p>


### **Assets**
- [Download pre-trained model](https://drive.google.com/drive/u/0/folders/1C3f9GGltuiRzsaexSyX36AlZBOeLn1Mc)
- [Table bank dataset](https://doc-analysis.github.io/tablebank-page/)

  
---
# Day 20:Training an Object Detector with Detectron2

This project walks through the steps required to train an object detector using **Detectron2**, specifically utilizing the **Alpaca dataset**. Key elements covered include annotations, YOLO format, dataset preparation, model configuration, and performance evaluation.

## Key Steps

### 1. Setting Up the Environment
- Install necessary dependencies such as `torch`, `detectron2`, and `opencv`.
- Create the essential Python files for training:
  - `train.py`: Main script for initiating and configuring training.
  - `util.py`: Utility functions for data handling and augmentation.
  - `loss.py`: Custom loss function to monitor model performance.

### 2. Dataset and Annotations
- Use the **Alpaca dataset** in YOLO format with annotations that include:
  - Class ID: The integer representing the object class (e.g., `1` for Alpacas).
  - Bounding box center: `(X, Y)` coordinates representing the center of the bounding box.
  - Width and Height: Dimensions of the bounding box.
- Make sure all annotation files are correctly formatted and consistent across the dataset.

### 3. Data Directory Structure
Ensure the data is organized for both training and validation:
  - `train/`: Contains images and annotation files for training.
    - Subfolders: 
      - `images/`: Contains image files.
      - `annotations/`: Contains the YOLO format annotation files.
  - `val/`: Contains images and annotations for validation.
    - Subfolders:
      - `images/`: Contains image files for validation.
      - `annotations/`: Contains validation annotation files.

### 4. Training Configuration
Configure `train.py` with the following parameters:
  - **Learning Rate**: Set the learning rate (e.g., `0.001`).
  - **Batch Size**: Choose an appropriate batch size (e.g., `16`).
  - **Iterations**: Set the number of iterations (e.g., `5000`).
  - **Pre-trained Model**: Use **RetinaNet R101** from Detectron2’s model zoo as a pre-trained backbone for object detection.

### 5. Model Zoo
Detectron2 provides a variety of pre-trained models in its model zoo, including:
  - **RetinaNet** for single-class and multi-class object detection.
  - **Faster R-CNN** for more complex detection tasks.
  - **Mask R-CNN** for segmentation tasks alongside detection.
Choose the most suitable model for your task to reduce training time and improve performance.

### 6. Training Process
- Begin training by running the `train.py` script:
  - The script loads the dataset and model, configures hyperparameters, and starts the training loop.
  - **Periodically save checkpoints** during training to allow for recovery in case of interruptions.
- For optimal training:
  - Run the training loop for several iterations (e.g., 5000 to 10000 iterations) based on your dataset size.

### 7. Validation and Loss Monitoring
- During training, regularly monitor training and validation loss to track progress:
  - **Training Loss**: Tracks how well the model fits the training data.
  - **Validation Loss**: Ensures the model generalizes well to unseen data.
- Save the loss metrics in a `metrics.json` file to track performance over time.
- Use a `plot_loss.py` script to visualize the loss curve and identify overfitting or underfitting.

### 8. Transition to Google Colab
Leverage Google Colab for faster training:
  - Upload the dataset and training scripts to Google Drive for easy access.
  - Run the training in Colab notebooks, which provides free access to GPU resources.
  - Use the Colab interface to execute training steps and evaluate model performance interactively.

### 9. Making Predictions
Once the model is trained:
  - Use the `predict.py` script to load the trained model and make predictions on test images.
  - For each image:
    - The model will output bounding boxes, class IDs, and confidence scores.
    - Visualize predictions to assess model accuracy and robustness.

## Insights
- **Structured Workflows**: Following a well-structured workflow and leveraging pre-trained models simplifies the object detection pipeline.
- **Validation and Monitoring**: Monitoring both training and validation losses helps ensure that the model is not overfitting and can generalize well.
- **Detectron2**: The toolset provided by Detectron2 is highly effective for object detection, offering flexibility, pre-trained models, and efficient training options.
- **Model Evaluation**: Using Colab’s GPU and effective validation strategies ensures rapid training, enabling timely iteration and improvement of the model.


--- 
# Day 21: Face Detection and Golden Ratio Analysis using face-api.js 

This project demonstrates real-time face detection and facial analysis using **face-api.js**, a JavaScript library. The goal is to showcase how facial landmarks are detected and how the Golden Ratio is calculated using these landmarks. This can be useful in various applications, including aesthetics analysis, biometrics, and more. The web app is built with **React**, providing a responsive user interface.

### Key Features and Functions:
1. **Real-Time Face Detection (Bounding Boxes)**:
   - **Goal**: Detect faces from webcam feed and display bounding boxes around each face.
   - **Key Functionality**: 
     - Uses **face-api.js** to detect faces in real-time.
     - Bounding boxes are drawn around faces with labels for each detected face.
     - Up to **multiple faces** can be detected simultaneously.
   
2. **Golden Ratio Calculation**:
   - **Goal**: Calculate the Golden Ratio using facial landmarks for aesthetic analysis.
   - **Key Calculation**:
     - Utilizes 68 facial landmarks (provided by **face-api.js**).
     - Focuses on the ratio between the forehead-to-chin distance and the eye-to-chin distance.
     - The formula: `Golden Ratio = Forehead-to-Chin Distance / Eye-to-Chin Distance`.
     - **Accuracy**: Results are displayed with 2 decimal places precision for better clarity.
     - **Example output**: A Golden Ratio of **1.86**, which indicates a fairly balanced proportion.
   
3. **Interactive Web Interface**:
   - **Buttons**:
     - **Start Capturing**: Start webcam feed and face detection.
     - **Stop Capturing**: Stop webcam feed and face detection.
     - **Analyze Face**: Trigger face detection and Golden Ratio calculation.
     - **Toggle Dark Mode**: Switch between light and dark modes for better UI customization.
   - **User Experience**:
     - Real-time updates are displayed, allowing users to see the bounding boxes as faces are detected and the Golden Ratio calculation for each face.
     - **Responsive Design**: Optimized for mobile and desktop use, ensuring a smooth user experience.

4. **Face Detection Algorithm**:
   - **Model**: Utilizes **ssdMobilenetv1** for face detection.
   - **Landmark Detection**: Uses **68 facial landmarks** for detailed analysis (such as eyes, nose, mouth, jawline).
   - **Canvas Drawing**: Bounding boxes and results are drawn directly on the HTML canvas element for real-time visualization.
   - **Detection Speed**: Capable of detecting faces at **30-40 FPS** on most modern devices.
   
5. **Real-Time Video Feed**:
   - **Webcam Access**: Access webcam via **getUserMedia** API.
   - **Video Dimensions**: Webcam feed is captured at **640x480 px** resolution for smooth processing.
   - **Canvas Size**: The canvas size is dynamically adjusted to match the video feed size for consistent detection.


## Project Setup and Workflow:
1. **Model Loading**:
   - The models (such as **ssdMobilenetv1**, **faceLandmark68Net**) are loaded from the **/models** folder.
   - Models are loaded asynchronously, and a loading message is displayed until all models are ready.
   - **Loading time**: Approximately **5-10 seconds** depending on network speed and device performance.

2. **User Interaction**:
   - Upon clicking **Start Capturing**, the webcam feed is initiated, and the face detection algorithm begins processing.
   - Clicking **Analyze Face** triggers the face detection and Golden Ratio calculation.
   - Bounding boxes are drawn around the detected faces, and Golden Ratio values are displayed in real-time.

3. **Golden Ratio Calculation**:
   - The Golden Ratio is calculated every time a face is detected.
   - Calculations are based on the following landmarks:
     - **Nose tip** (point 6 from the nose landmarks) for eye-to-chin distance.
     - **Jaw outline** (point 8 from the jaw outline landmarks) for forehead-to-chin distance.
   - The result is rounded to **2 decimal places** for clarity, e.g., **1.86**.

## Technologies Used:
- **face-api.js** for face detection, landmark identification, and drawing bounding boxes.
- **React.js** for the frontend user interface.
- **HTML5 Canvas** for real-time bounding box drawing on the video feed.
- **JavaScript (ES6+)** for asynchronous model loading and real-time video feed handling.
- **CSS** for styling, including responsive design and dark mode support.

## Output:
- **Face Detection**:
  - Bounding boxes drawn around each detected face.
  - Real-time feedback on the detected faces.
  - **Multiple faces** can be detected simultaneously, with bounding boxes and labels for each face.
  
- **Golden Ratio Calculation**:
  - Real-time calculation of the Golden Ratio, displayed in the UI with precision (e.g., **1.86**).
  - Users can observe how the facial proportion relates to the Golden Ratio for aesthetic analysis.

## Assets and References:
- **face-api.js Documentation**: [https://github.com/justadudewhohacks/face-api.js](https://github.com/justadudewhohacks/face-api.js)
- **React Documentation**: [https://reactjs.org/](https://reactjs.org/)
---
# Day 22: Building a Face Attendance System with Face Recognition

This project focuses on constructing a **Face Attendance System** using **face recognition** technology. The system provides a seamless way to register and log attendance through facial recognition, integrating computer vision principles and machine learning for an interactive user experience [link.](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Face%20attendance%20with%20python)

## **Key Features**

1. **User Registration**  
   - Allows new users to register by capturing a selfie and entering their name.
2. **User Login**  
   - Enables registered users to log in through real-time facial recognition.
3. **Webcam Integration**  
   - Displays a live video feed for user interaction, supporting login and registration actions.
4. **Attendance Logging**  
   - Records the user’s name and timestamp upon successful login, creating a detailed attendance log.
5. **Error Handling**  
   - Prompts unrecognized users to either register or retry logging in, ensuring a smooth user experience.

## **System Overview**

- **Interface Design**:  
  - A user-friendly interface with buttons for login and registration.
  - Webcam feed updates in real-time for interaction.
- **Registration Process**:  
  1. Users click "Register New User" to start the process.  
  2. The system accesses the webcam for capturing a selfie.  
  3. Users save the image along with their name to a database.  
- **Login Process**:  
  1. Registered users click "Login" to start recognition.  
  2. The system compares live webcam frames with stored images.  
  3. A match welcomes the user; otherwise, a notification is displayed.  

## **Implementation Steps**

1. **Environment Setup**:  
   - Install dependencies with `pip install opencv-python face-recognition`.  
   - Ensure the system can access the webcam securely.
2. **Interface Development**:  
   - Use Tkinter to create a main window with buttons for login and registration.  
   - Embed the live webcam feed into the GUI for user interaction.
3. **Registration Process**:  
   - Capture a selfie using the webcam.  
   - Save the image along with the user’s name in a designated directory.
4. **Facial Recognition**:  
   - Compare live frames with stored images using the face recognition library.  
   - Log attendance by appending the user’s name and timestamp to a text file.
     
## Assets
  - [face_recognition model](https://github.com/ageitgey/face_recognition)
  - [Silent-Face-Anti-Spoofing](https://github.com/minivision-ai/Silent-Face-Anti-Spoofing/tree/master)
  - [using webapp](https://www.youtube.com/watch?v=yWmW5uEtNws)
---
# Day 23: Setting Up an Intruder Detection System Using AWS

In this project, we set up an **intruder detection system** leveraging several Amazon Web Services (AWS) components. The system integrates **real-time video streaming**, **machine learning**, and **cloud-based storage solutions** to enhance building security. By utilizing **Amazon Kinesis Video Streams**, **Amazon Rekognition**, **DynamoDB**, and **SNS (Simple Notification Service)**, this project delivers a scalable, secure, and efficient security solution.

## **Key Features**

1. **Real-Time Video Streaming**  
   - Webcam captures video and streams it in real-time using **Amazon Kinesis Video Streams**.

2. **Intruder Detection**  
   - **Amazon Rekognition** analyzes each frame for human presence.

3. **Data Storage**  
   - Video frames and detection data are backed up to **Amazon S3** and stored in **DynamoDB**.

4. **Notification System**  
   - **SNS** sends email alerts to users upon detecting an intruder.

## **System Architecture**

- **Webcam**: Captures video for analysis.  
- **Kinesis Video Streams**: Streams the captured video to the cloud for processing.  
- **EC2 Instances**: Host applications for backup and intruder detection.  
- **Amazon Rekognition**: Detects human presence in video frames.  
- **S3 (Simple Storage Service)**: Stores video frames and backup data.  
- **DynamoDB**: Keeps records of intruder detection events.  
- **SNS (Simple Notification Service)**: Sends email alerts to users when an intruder is detected.

## **System Functionality**

1. **Real-Time Streaming**  
   - The webcam streams video to **Kinesis Video Streams**.  
   - The backup consumer saves every frame to **S3** for future reference.

2. **Intruder Detection**  
   - **Amazon Rekognition** checks each video frame for human presence.  
   - The system continuously analyzes frames, processing one frame every 30 frames to optimize performance.

3. **Notification System**  
   - Upon detecting an intruder, **SNS** triggers an email alert to registered users.

## **Step-by-Step Implementation**

1. **Configuring the Video Producer**  
   - Use the **Kinesis Video Streams Producer SDK** to stream data from the webcam.  
   - A new video stream named "Intruder detection tutorial video stream" is created in the AWS Management Console.

2. **Setting Up IAM User and Access Keys**  
   - Create an IAM user with full access to **Kinesis Video Streams**.  
   - Generate access keys for authenticating API calls.

3. **Implementing the Backup Consumer**  
   - Launch an **EC2 instance (t2.medium)** to handle the backup process.  
   - Create an **S3 bucket** ("Intruder detection tutorial bucket") to store video frames.  
   - Set up the backup consumer to receive and store frames from Kinesis in the S3 bucket.

4. **Developing the Intruder Detection Consumer**  
   - Launch another **EC2 instance** to handle intruder detection.  
   - Create IAM roles to grant permissions for accessing **Rekognition**, **S3**, and **DynamoDB**.  
   - Use **Amazon Rekognition's `detect_labels` function** to analyze frames and detect human presence.

## **Technical Implementation Details**

- **Streaming and Processing**  
   - Video frames are streamed from the webcam and backed up to **S3**.  
   - The intruder detection consumer processes frames at one frame every 30 frames to balance performance and efficiency.

- **Amazon Rekognition Integration**  
   - Frames are processed and converted into a format compatible with **Rekognition**.  
   - The system checks for the label "person" to confirm the presence of an intruder.

- **Data Storage and Alerts**  
   - Detected frames are stored in a dedicated "detections" folder in **S3**.  
   - Relevant detection data, including bounding boxes and frame numbers, are stored in **DynamoDB**.  
   - **SNS** is used to send email notifications to users when an intruder is detected.

## **Key Takeaways**

- **AWS Components**: Knowledge of services like **Kinesis**, **Rekognition**, **S3**, and **DynamoDB** is essential for building cloud-based security systems.
- **Real-Time Processing**: The ability to process data in real-time is critical for security applications.
- **Cost Management**: Understanding the costs associated with AWS services is important for sustainable system implementation.
- **Scalability**: Cloud solutions provide scalability, making it easy to expand security systems as needed.


---
# Day 24: Building a Web Application for Image Analysis with Python, Streamlit, and Langchain

This project demonstrates how to create a web application that processes user-uploaded images, detects objects, and answers questions about image content. By leveraging **Streamlit** for the user interface and **Langchain** for managing interactions with a large language model, this app showcases the seamless integration of computer vision and natural language processing technologies. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Image%20webapp%20-%20Langchain)

## Features

- **Image Uploads**: Users can upload images for analysis through a web interface.
- **Dynamic Interactions**: Users can ask questions about image content or request detailed object detection.
- **Object Detection**: Uses pre-trained models like **Facebook DETR ResNet50** to identify objects in images with bounding boxes and confidence scores.
- **Image Captioning**: Generates descriptive captions for uploaded images using **Salesforce BLIP**.
- **Conversational Memory**: Retains previous interactions to provide dynamic and context-aware responses.
- **Streamlit Interface**: Simple and intuitive UI for uploading images and interacting with the model.

## Implementation Steps

### 1. Setting Up the Web Application
- Build a user interface with **Streamlit** for image uploads and text inputs.
- Initialize a Langchain agent to handle queries and interact with the image-processing tools.

### 2. Adding Image Processing Functions
- **get_image_caption**: Generates captions for images using a pre-trained BLIP model.
- **detect_objects**: Detects objects in images, highlighting them with bounding boxes and confidence scores.

### 3. Creating Langchain Tools
- **Image Caption Tool**: Captures image descriptions using the BLIP model.
- **Object Detection Tool**: Processes images to identify objects and provides detailed information about them.

### 4. Managing API Interactions
- Implements retry logic to handle **RateLimitError**, ensuring smooth communication with OpenAI APIs.
- Logs errors and warnings to facilitate debugging and monitoring.

--- 
# Day 25: Dreambooth Image Generation - Python Application

This project involves building a Python application that leverages **Dreambooth** to generate personalized and stylized images based on user-provided inputs. The project integrates **AWS services** like S3 and SQS for efficient task management and storage and utilizes **RunPod** for high-performance model training. The goal is to provide a scalable platform for creative image generation using deep learning.

## Project Features

### Personalized Image Generation
- Train custom models with user-provided image datasets.
- Generate stylized images based on predefined styles like "Viking" or "Jedi."

### AWS Integration
- **S3 Bucket**:
  - Store input images and generated results securely.
- **SQS Queue**:
  - Manage training and inference tasks for efficient processing.

### Prediction and Training Workflow
- Automates the process of uploading images, triggering training, and retrieving results.
- Ensures task deduplication with SQS for seamless operations.

### Cloud-Based Training
- **RunPod GPU Pods**:
  - Leverages high-performance GPUs (e.g., RTX A6000) for model training.

### Dynamic Interaction
- Users can enter custom prompts for unique outputs, enhancing creativity.
- Generates multiple images for selected styles with varying confidence levels.

### User-Friendly Interface
- Simple Python application with clear configurations.
- Integration with local environments and cloud services for a smooth experience.
## Output
- Generate stylized images from user-provided inputs.
- View results directly in the local application or fetch from **S3**.

## Assets
- Clone the **Dreambooth Repository** from GitHub.
- AWS setup for **S3 Bucket**, **SQS Queue**, and **IAM User**.
- Deploy **RunPod** instances for GPU-accelerated processing.



---
# Day 26: Real-Time Face Recognition and Attendance System

A real-time face recognition system to automate attendance management using Flask, OpenCV, and machine learning. [link](https://github.com/Ni2thin/30DaysOfComputerVision/tree/main/Projects/Face%20attendance%20with%20GUI)

## **Features**

- **Face Detection**: Detects faces in real-time using OpenCV's Haar cascades.
- **Face Recognition**: Identifies individuals using a KNN model trained with facial features.
- **Attendance Tracking**: Automatically logs attendance with names, roll numbers, and timestamps.
- **User Management**: Add, list, or delete users and retrain the model.
- **Real-Time Updates**: Captures and processes faces live, showing progress during user registration.
- **Interactive UI**: Simple web interface for managing users and viewing attendance.

## **Technical Details**

- **Model Training**: KNN classifier trained on resized, flattened face images.
- **Attendance Storage**: Daily CSV files to record attendance.
- **Web Routes**:
  - `/`: Displays daily attendance.
  - `/listusers`: Lists registered users.
  - `/add`: Registers new users by capturing face images.
  - `/deleteuser`: Deletes user data and retrains the model.
  - `/start`: Starts the face recognition process.

## **Output**
<img src="https://github.com/Ni2thin/30DaysOfComputerVision/blob/main/Projects/Face%20attendance%20with%20GUI/attendance.png" width="1050" height="500"/>

## **Assets**
- Haar cascade for face detection (`haarcascade_frontalface_default.xml`).
- Trained model (`face_recognition_model.pkl`).
- Attendance data stored in CSV files.

---
# Day 27:




